{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bd79a-353d-495a-b6c3-6f6cbde4e8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data directory: output_lexical_metrics\n",
      " - Subfolder 'output_lexical_metrics/cc': FOUND\n",
      " - Subfolder 'output_lexical_metrics/cd': FOUND\n",
      "\n",
      "Loading CSV files...\n",
      " * 54 files in 'cc'\n",
      " * 54 files in 'cd'\n",
      "\n",
      "Total samples: 108 (cc=54, cd=54)\n",
      "\n",
      "Checking for missing values per feature:\n",
      "all_freq_raw_mean                 1\n",
      "all_freq_log_mean                 1\n",
      "all_cd_raw_mean                   1\n",
      "all_cd_log_mean                   1\n",
      "all_concreteness_m_mean           1\n",
      "all_concreteness_sd_mean          1\n",
      "all_mean_cos_mean                 1\n",
      "all_SemD_mean                     1\n",
      "all_BNC_wordcount_mean            1\n",
      "all_BNC_contexts_mean             1\n",
      "all_BNC_freq_mean                 1\n",
      "all_lg_BNC_freq_mean              1\n",
      "all_phonemes_mean                 1\n",
      "content_freq_raw_mean             1\n",
      "content_freq_log_mean             1\n",
      "content_cd_raw_mean               1\n",
      "content_cd_log_mean               1\n",
      "content_concreteness_m_mean       1\n",
      "content_concreteness_sd_mean      1\n",
      "content_mean_cos_mean             1\n",
      "content_SemD_mean                 1\n",
      "content_BNC_wordcount_mean        1\n",
      "content_BNC_contexts_mean         1\n",
      "content_BNC_freq_mean             1\n",
      "content_lg_BNC_freq_mean          1\n",
      "content_phonemes_mean             1\n",
      "noun_freq_raw_mean                1\n",
      "noun_freq_log_mean                1\n",
      "noun_cd_raw_mean                  1\n",
      "noun_cd_log_mean                  1\n",
      "noun_concreteness_m_mean          2\n",
      "noun_concreteness_sd_mean         2\n",
      "noun_mean_cos_mean                1\n",
      "noun_SemD_mean                    1\n",
      "noun_BNC_wordcount_mean           1\n",
      "noun_BNC_contexts_mean            1\n",
      "noun_BNC_freq_mean                1\n",
      "noun_lg_BNC_freq_mean             1\n",
      "noun_phonemes_mean                1\n",
      "POS_DET_per100                    1\n",
      "POS_NOUN_per100                   1\n",
      "POS_CCONJ_per100                 11\n",
      "POS_VERB_per100                   1\n",
      "POS_PRON_per100                   1\n",
      "POS_ADP_per100                    1\n",
      "POS_PROPN_per100                 73\n",
      "POS_INTJ_per100                  17\n",
      "POS_AUX_per100                    8\n",
      "POS_ADJ_per100                   10\n",
      "POS_SCONJ_per100                 45\n",
      "POS_PART_per100                  23\n",
      "POS_ADV_per100                    6\n",
      "POS_NUM_per100                   73\n",
      "POS_X_per100                    104\n",
      "POS_PUNCT_per100                107\n",
      "dtype: int64\n",
      "\n",
      "Imputing missing values using column means...\n",
      "Imputation complete.\n",
      "\n",
      "Splitting data (80% train, 20% test)...\n",
      " Training samples: 86\n",
      " Test samples:     22\n",
      "\n",
      "Standardizing features...\n",
      "Standardization complete.\n",
      "\n",
      "Initializing MLPClassifier...\n",
      "\n",
      "Starting model training...\n",
      "Iteration 1, loss = 0.71576645\n",
      "Iteration 2, loss = 0.66995494\n",
      "Iteration 3, loss = 0.63099145\n",
      "Iteration 4, loss = 0.59821859\n",
      "Iteration 5, loss = 0.57098286\n",
      "Iteration 6, loss = 0.54806046\n",
      "Iteration 7, loss = 0.52831523\n",
      "Iteration 8, loss = 0.51087957\n",
      "Iteration 9, loss = 0.49524779\n",
      "Iteration 10, loss = 0.48077029\n",
      "Iteration 11, loss = 0.46728052\n",
      "Iteration 12, loss = 0.45447150\n",
      "Iteration 13, loss = 0.44228710\n",
      "Iteration 14, loss = 0.43067430\n",
      "Iteration 15, loss = 0.41944535\n",
      "Iteration 16, loss = 0.40811991\n",
      "Iteration 17, loss = 0.39677913\n",
      "Iteration 18, loss = 0.38553081\n",
      "Iteration 19, loss = 0.37449877\n",
      "Iteration 20, loss = 0.36389834\n",
      "Iteration 21, loss = 0.35361406\n",
      "Iteration 22, loss = 0.34353971\n",
      "Iteration 23, loss = 0.33370996\n",
      "Iteration 24, loss = 0.32400007\n",
      "Iteration 25, loss = 0.31463652\n",
      "Iteration 26, loss = 0.30551965\n",
      "Iteration 27, loss = 0.29656874\n",
      "Iteration 28, loss = 0.28767335\n",
      "Iteration 29, loss = 0.27878108\n",
      "Iteration 30, loss = 0.26994869\n",
      "Iteration 31, loss = 0.26131338\n",
      "Iteration 32, loss = 0.25279744\n",
      "Iteration 33, loss = 0.24438211\n",
      "Iteration 34, loss = 0.23611381\n",
      "Iteration 35, loss = 0.22799210\n",
      "Iteration 36, loss = 0.22009579\n",
      "Iteration 37, loss = 0.21234806\n",
      "Iteration 38, loss = 0.20481232\n",
      "Iteration 39, loss = 0.19757674\n",
      "Iteration 40, loss = 0.19051299\n",
      "Iteration 41, loss = 0.18363583\n",
      "Iteration 42, loss = 0.17685298\n",
      "Iteration 43, loss = 0.17024452\n",
      "Iteration 44, loss = 0.16373307\n",
      "Iteration 45, loss = 0.15739428\n",
      "Iteration 46, loss = 0.15128481\n",
      "Iteration 47, loss = 0.14533235\n",
      "Iteration 48, loss = 0.13948213\n",
      "Iteration 49, loss = 0.13382970\n",
      "Iteration 50, loss = 0.12838173\n",
      "Iteration 51, loss = 0.12313642\n",
      "Iteration 52, loss = 0.11806279\n",
      "Iteration 53, loss = 0.11313647\n",
      "Iteration 54, loss = 0.10835696\n",
      "Iteration 55, loss = 0.10373207\n",
      "Iteration 56, loss = 0.09926621\n",
      "Iteration 57, loss = 0.09498940\n",
      "Iteration 58, loss = 0.09092137\n",
      "Iteration 59, loss = 0.08700789\n",
      "Iteration 60, loss = 0.08325321\n",
      "Iteration 61, loss = 0.07966631\n",
      "Iteration 62, loss = 0.07621936\n",
      "Iteration 63, loss = 0.07290784\n",
      "Iteration 64, loss = 0.06973011\n",
      "Iteration 65, loss = 0.06666314\n",
      "Iteration 66, loss = 0.06372754\n",
      "Iteration 67, loss = 0.06091046\n",
      "Iteration 68, loss = 0.05822133\n",
      "Iteration 69, loss = 0.05564578\n",
      "Iteration 70, loss = 0.05318516\n",
      "Iteration 71, loss = 0.05084934\n",
      "Iteration 72, loss = 0.04862199\n",
      "Iteration 73, loss = 0.04645906\n",
      "Iteration 74, loss = 0.04438889\n",
      "Iteration 75, loss = 0.04241096\n",
      "Iteration 76, loss = 0.04052694\n",
      "Iteration 77, loss = 0.03873650\n",
      "Iteration 78, loss = 0.03704083\n",
      "Iteration 79, loss = 0.03543041\n",
      "Iteration 80, loss = 0.03391169\n",
      "Iteration 81, loss = 0.03246178\n",
      "Iteration 82, loss = 0.03108611\n",
      "Iteration 83, loss = 0.02977785\n",
      "Iteration 84, loss = 0.02852753\n",
      "Iteration 85, loss = 0.02733855\n",
      "Iteration 86, loss = 0.02621119\n",
      "Iteration 87, loss = 0.02513944\n",
      "Iteration 88, loss = 0.02411966\n",
      "Iteration 89, loss = 0.02314839\n",
      "Iteration 90, loss = 0.02222747\n",
      "Iteration 91, loss = 0.02135484\n",
      "Iteration 92, loss = 0.02052410\n",
      "Iteration 93, loss = 0.01973585\n",
      "Iteration 94, loss = 0.01899729\n",
      "Iteration 95, loss = 0.01829782\n",
      "Iteration 96, loss = 0.01763364\n",
      "Iteration 97, loss = 0.01700451\n",
      "Iteration 98, loss = 0.01640905\n",
      "Iteration 99, loss = 0.01584488\n",
      "Iteration 100, loss = 0.01530683\n",
      "Iteration 101, loss = 0.01479067\n",
      "Iteration 102, loss = 0.01429762\n",
      "Iteration 103, loss = 0.01382999\n",
      "Iteration 104, loss = 0.01338782\n",
      "Iteration 105, loss = 0.01296661\n",
      "Iteration 106, loss = 0.01256338\n",
      "Iteration 107, loss = 0.01218035\n",
      "Iteration 108, loss = 0.01181579\n",
      "Iteration 109, loss = 0.01146719\n",
      "Iteration 110, loss = 0.01113353\n",
      "Iteration 111, loss = 0.01081392\n",
      "Iteration 112, loss = 0.01050689\n",
      "Iteration 113, loss = 0.01021154\n",
      "Iteration 114, loss = 0.00992945\n",
      "Iteration 115, loss = 0.00965961\n",
      "Iteration 116, loss = 0.00940130\n",
      "Iteration 117, loss = 0.00915241\n",
      "Iteration 118, loss = 0.00891332\n",
      "Iteration 119, loss = 0.00868390\n",
      "Iteration 120, loss = 0.00846266\n",
      "Iteration 121, loss = 0.00824960\n",
      "Iteration 122, loss = 0.00804556\n",
      "Iteration 123, loss = 0.00784966\n",
      "Iteration 124, loss = 0.00766076\n",
      "Iteration 125, loss = 0.00747871\n",
      "Iteration 126, loss = 0.00730349\n",
      "Iteration 127, loss = 0.00713465\n",
      "Iteration 128, loss = 0.00697218\n",
      "Iteration 129, loss = 0.00681523\n",
      "Iteration 130, loss = 0.00666349\n",
      "Iteration 131, loss = 0.00651746\n",
      "Iteration 132, loss = 0.00637700\n",
      "Iteration 133, loss = 0.00624173\n",
      "Iteration 134, loss = 0.00611100\n",
      "Iteration 135, loss = 0.00598448\n",
      "Iteration 136, loss = 0.00586232\n",
      "Iteration 137, loss = 0.00574413\n",
      "Iteration 138, loss = 0.00562982\n",
      "Iteration 139, loss = 0.00551866\n",
      "Iteration 140, loss = 0.00541055\n",
      "Iteration 141, loss = 0.00530551\n",
      "Iteration 142, loss = 0.00520381\n",
      "Iteration 143, loss = 0.00510502\n",
      "Iteration 144, loss = 0.00500935\n",
      "Iteration 145, loss = 0.00491657\n",
      "Iteration 146, loss = 0.00482612\n",
      "Iteration 147, loss = 0.00473817\n",
      "Iteration 148, loss = 0.00465246\n",
      "Iteration 149, loss = 0.00456937\n",
      "Iteration 150, loss = 0.00448842\n",
      "Iteration 151, loss = 0.00440964\n",
      "Iteration 152, loss = 0.00433346\n",
      "Iteration 153, loss = 0.00425945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training finished.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        11\n",
      "           1       0.75      0.82      0.78        11\n",
      "\n",
      "    accuracy                           0.77        22\n",
      "   macro avg       0.78      0.77      0.77        22\n",
      "weighted avg       0.78      0.77      0.77        22\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8 3]\n",
      " [2 9]]\n",
      "\n",
      "Final training loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Configure your data directory\n",
    "base_dir = 'output_lexical_metrics'\n",
    "\n",
    "# 2. Verify directory structure\n",
    "print(f\"Checking data directory: {base_dir}\")\n",
    "if not os.path.isdir(base_dir):\n",
    "    raise FileNotFoundError(f\"Data directory '{base_dir}' does not exist.\")\n",
    "for subgroup in ['cc', 'cd']:\n",
    "    subgroup_path = os.path.join(base_dir, subgroup)\n",
    "    print(f\" - Subfolder '{subgroup_path}': {'FOUND' if os.path.isdir(subgroup_path) else 'MISSING'}\")\n",
    "    if not os.path.isdir(subgroup_path):\n",
    "        raise FileNotFoundError(f\"Expected subfolder '{subgroup_path}' not found.\")\n",
    "\n",
    "# 3. Load CSV metrics and labels\n",
    "print(\"\\nLoading CSV files...\")\n",
    "data_rows, labels = [], []\n",
    "for label in ['cc', 'cd']:\n",
    "    folder = os.path.join(base_dir, label)\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith('.csv')]\n",
    "    print(f\" * {len(files)} files in '{label}'\")\n",
    "    for fname in files:\n",
    "        df = pd.read_csv(os.path.join(folder, fname))\n",
    "        data_rows.append(df.iloc[0])\n",
    "        labels.append(label)\n",
    "\n",
    "# 4. Build DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "df['label'] = labels\n",
    "print(f\"\\nTotal samples: {len(df)} (cc={df.label.value_counts()['cc']}, cd={df.label.value_counts()['cd']})\")\n",
    "\n",
    "# 5. Separate features/target and check for NaNs\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].map({'cc': 0, 'cd': 1})\n",
    "\n",
    "print(\"\\nChecking for missing values per feature:\")\n",
    "missing = X.isnull().sum()\n",
    "print(missing[missing > 0] if missing.any() else \"No missing values detected!\")\n",
    "\n",
    "# 6. Impute missing values (if any)\n",
    "if missing.any():\n",
    "    print(\"\\nImputing missing values using column means...\")\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    print(\"Imputation complete.\")\n",
    "\n",
    "# 7. Train/test split\n",
    "print(\"\\nSplitting data (80% train, 20% test)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\" Training samples: {X_train.shape[0]}\")\n",
    "print(f\" Test samples:     {X_test.shape[0]}\")\n",
    "\n",
    "# 8. Standardize features\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "print(\"Standardization complete.\")\n",
    "\n",
    "# 9. Initialize and train MLP (with verbose output)\n",
    "print(\"\\nInitializing MLPClassifier...\")\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 10. Evaluate\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\nFinal training loss: {mlp.loss_curve_[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
