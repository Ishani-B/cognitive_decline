{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9f856-ff93-4f65-879c-9f71ea2e08dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data directory: output_lexical_metrics\n",
      " - Subfolder 'output_lexical_metrics/cc': FOUND\n",
      " - Subfolder 'output_lexical_metrics/cd': FOUND\n",
      "\n",
      "Loading CSV files...\n",
      " * 54 files in 'cc'\n",
      " * 54 files in 'cd'\n",
      "\n",
      "Total samples: 108 (cc=54, cd=54)\n",
      "\n",
      "Missing values per feature:\n",
      "all_freq_raw_mean                 1\n",
      "all_freq_log_mean                 1\n",
      "all_cd_raw_mean                   1\n",
      "all_cd_log_mean                   1\n",
      "all_concreteness_m_mean           1\n",
      "all_concreteness_sd_mean          1\n",
      "all_mean_cos_mean                 1\n",
      "all_SemD_mean                     1\n",
      "all_BNC_wordcount_mean            1\n",
      "all_BNC_contexts_mean             1\n",
      "all_BNC_freq_mean                 1\n",
      "all_lg_BNC_freq_mean              1\n",
      "all_phonemes_mean                 1\n",
      "content_freq_raw_mean             1\n",
      "content_freq_log_mean             1\n",
      "content_cd_raw_mean               1\n",
      "content_cd_log_mean               1\n",
      "content_concreteness_m_mean       1\n",
      "content_concreteness_sd_mean      1\n",
      "content_mean_cos_mean             1\n",
      "content_SemD_mean                 1\n",
      "content_BNC_wordcount_mean        1\n",
      "content_BNC_contexts_mean         1\n",
      "content_BNC_freq_mean             1\n",
      "content_lg_BNC_freq_mean          1\n",
      "content_phonemes_mean             1\n",
      "noun_freq_raw_mean                1\n",
      "noun_freq_log_mean                1\n",
      "noun_cd_raw_mean                  1\n",
      "noun_cd_log_mean                  1\n",
      "noun_concreteness_m_mean          2\n",
      "noun_concreteness_sd_mean         2\n",
      "noun_mean_cos_mean                1\n",
      "noun_SemD_mean                    1\n",
      "noun_BNC_wordcount_mean           1\n",
      "noun_BNC_contexts_mean            1\n",
      "noun_BNC_freq_mean                1\n",
      "noun_lg_BNC_freq_mean             1\n",
      "noun_phonemes_mean                1\n",
      "POS_DET_per100                    1\n",
      "POS_NOUN_per100                   1\n",
      "POS_CCONJ_per100                 11\n",
      "POS_VERB_per100                   1\n",
      "POS_PRON_per100                   1\n",
      "POS_ADP_per100                    1\n",
      "POS_PROPN_per100                 73\n",
      "POS_INTJ_per100                  17\n",
      "POS_AUX_per100                    8\n",
      "POS_ADJ_per100                   10\n",
      "POS_SCONJ_per100                 45\n",
      "POS_PART_per100                  23\n",
      "POS_ADV_per100                    6\n",
      "POS_NUM_per100                   73\n",
      "POS_X_per100                    104\n",
      "POS_PUNCT_per100                107\n",
      "dtype: int64\n",
      "\n",
      "Imputing missing values using KNNImputer (k=5)...\n",
      "Imputation complete. No more missing values: True\n",
      "\n",
      "Splitting data (80% train, 20% test)...\n",
      " Training samples: 86\n",
      " Test samples:     22\n",
      "\n",
      "Standardizing features...\n",
      "Standardization complete.\n",
      "\n",
      "Initializing MLPClassifier...\n",
      "\n",
      "Starting model training...\n",
      "Iteration 1, loss = 0.71270148\n",
      "Iteration 2, loss = 0.66625644\n",
      "Iteration 3, loss = 0.62781774\n",
      "Iteration 4, loss = 0.59521132\n",
      "Iteration 5, loss = 0.56769063\n",
      "Iteration 6, loss = 0.54446218\n",
      "Iteration 7, loss = 0.52444020\n",
      "Iteration 8, loss = 0.50683639\n",
      "Iteration 9, loss = 0.49073578\n",
      "Iteration 10, loss = 0.47592908\n",
      "Iteration 11, loss = 0.46218981\n",
      "Iteration 12, loss = 0.44948576\n",
      "Iteration 13, loss = 0.43742052\n",
      "Iteration 14, loss = 0.42583675\n",
      "Iteration 15, loss = 0.41434389\n",
      "Iteration 16, loss = 0.40278724\n",
      "Iteration 17, loss = 0.39120239\n",
      "Iteration 18, loss = 0.37969667\n",
      "Iteration 19, loss = 0.36838454\n",
      "Iteration 20, loss = 0.35736337\n",
      "Iteration 21, loss = 0.34660658\n",
      "Iteration 22, loss = 0.33609006\n",
      "Iteration 23, loss = 0.32579841\n",
      "Iteration 24, loss = 0.31571707\n",
      "Iteration 25, loss = 0.30584749\n",
      "Iteration 26, loss = 0.29623795\n",
      "Iteration 27, loss = 0.28677834\n",
      "Iteration 28, loss = 0.27750108\n",
      "Iteration 29, loss = 0.26839349\n",
      "Iteration 30, loss = 0.25928330\n",
      "Iteration 31, loss = 0.25028468\n",
      "Iteration 32, loss = 0.24145664\n",
      "Iteration 33, loss = 0.23277629\n",
      "Iteration 34, loss = 0.22431819\n",
      "Iteration 35, loss = 0.21606921\n",
      "Iteration 36, loss = 0.20797859\n",
      "Iteration 37, loss = 0.20013901\n",
      "Iteration 38, loss = 0.19247586\n",
      "Iteration 39, loss = 0.18507804\n",
      "Iteration 40, loss = 0.17787565\n",
      "Iteration 41, loss = 0.17087167\n",
      "Iteration 42, loss = 0.16402616\n",
      "Iteration 43, loss = 0.15727465\n",
      "Iteration 44, loss = 0.15065270\n",
      "Iteration 45, loss = 0.14420188\n",
      "Iteration 46, loss = 0.13798512\n",
      "Iteration 47, loss = 0.13203628\n",
      "Iteration 48, loss = 0.12630128\n",
      "Iteration 49, loss = 0.12075742\n",
      "Iteration 50, loss = 0.11543623\n",
      "Iteration 51, loss = 0.11028238\n",
      "Iteration 52, loss = 0.10527320\n",
      "Iteration 53, loss = 0.10038031\n",
      "Iteration 54, loss = 0.09568505\n",
      "Iteration 55, loss = 0.09119849\n",
      "Iteration 56, loss = 0.08687961\n",
      "Iteration 57, loss = 0.08275390\n",
      "Iteration 58, loss = 0.07882136\n",
      "Iteration 59, loss = 0.07507735\n",
      "Iteration 60, loss = 0.07149735\n",
      "Iteration 61, loss = 0.06809370\n",
      "Iteration 62, loss = 0.06483320\n",
      "Iteration 63, loss = 0.06172120\n",
      "Iteration 64, loss = 0.05875574\n",
      "Iteration 65, loss = 0.05594004\n",
      "Iteration 66, loss = 0.05326556\n",
      "Iteration 67, loss = 0.05071580\n",
      "Iteration 68, loss = 0.04831646\n",
      "Iteration 69, loss = 0.04602591\n",
      "Iteration 70, loss = 0.04382964\n",
      "Iteration 71, loss = 0.04174144\n",
      "Iteration 72, loss = 0.03976515\n",
      "Iteration 73, loss = 0.03791431\n",
      "Iteration 74, loss = 0.03616124\n",
      "Iteration 75, loss = 0.03449809\n",
      "Iteration 76, loss = 0.03292803\n",
      "Iteration 77, loss = 0.03143696\n",
      "Iteration 78, loss = 0.03001544\n",
      "Iteration 79, loss = 0.02867148\n",
      "Iteration 80, loss = 0.02740391\n",
      "Iteration 81, loss = 0.02620850\n",
      "Iteration 82, loss = 0.02507949\n",
      "Iteration 83, loss = 0.02401736\n",
      "Iteration 84, loss = 0.02301702\n",
      "Iteration 85, loss = 0.02207635\n",
      "Iteration 86, loss = 0.02118243\n",
      "Iteration 87, loss = 0.02033490\n",
      "Iteration 88, loss = 0.01953302\n",
      "Iteration 89, loss = 0.01877084\n",
      "Iteration 90, loss = 0.01805083\n",
      "Iteration 91, loss = 0.01736799\n",
      "Iteration 92, loss = 0.01672571\n",
      "Iteration 93, loss = 0.01611857\n",
      "Iteration 94, loss = 0.01553953\n",
      "Iteration 95, loss = 0.01499148\n",
      "Iteration 96, loss = 0.01447098\n",
      "Iteration 97, loss = 0.01397376\n",
      "Iteration 98, loss = 0.01349914\n",
      "Iteration 99, loss = 0.01305023\n",
      "Iteration 100, loss = 0.01262413\n",
      "Iteration 101, loss = 0.01221703\n",
      "Iteration 102, loss = 0.01182986\n",
      "Iteration 103, loss = 0.01146069\n",
      "Iteration 104, loss = 0.01110740\n",
      "Iteration 105, loss = 0.01076858\n",
      "Iteration 106, loss = 0.01044445\n",
      "Iteration 107, loss = 0.01013735\n",
      "Iteration 108, loss = 0.00984587\n",
      "Iteration 109, loss = 0.00956593\n",
      "Iteration 110, loss = 0.00929874\n",
      "Iteration 111, loss = 0.00904398\n",
      "Iteration 112, loss = 0.00879906\n",
      "Iteration 113, loss = 0.00856369\n",
      "Iteration 114, loss = 0.00833824\n",
      "Iteration 115, loss = 0.00812084\n",
      "Iteration 116, loss = 0.00791205\n",
      "Iteration 117, loss = 0.00771144\n",
      "Iteration 118, loss = 0.00751938\n",
      "Iteration 119, loss = 0.00733598\n",
      "Iteration 120, loss = 0.00715965\n",
      "Iteration 121, loss = 0.00698983\n",
      "Iteration 122, loss = 0.00682567\n",
      "Iteration 123, loss = 0.00666788\n",
      "Iteration 124, loss = 0.00651572\n",
      "Iteration 125, loss = 0.00636936\n",
      "Iteration 126, loss = 0.00622889\n",
      "Iteration 127, loss = 0.00609274\n",
      "Iteration 128, loss = 0.00596222\n",
      "Iteration 129, loss = 0.00583654\n",
      "Iteration 130, loss = 0.00571509\n",
      "Iteration 131, loss = 0.00559735\n",
      "Iteration 132, loss = 0.00548362\n",
      "Iteration 133, loss = 0.00537351\n",
      "Iteration 134, loss = 0.00526735\n",
      "Iteration 135, loss = 0.00516480\n",
      "Iteration 136, loss = 0.00506490\n",
      "Iteration 137, loss = 0.00496832\n",
      "Iteration 138, loss = 0.00487463\n",
      "Iteration 139, loss = 0.00478416\n",
      "Iteration 140, loss = 0.00469646\n",
      "Iteration 141, loss = 0.00461143\n",
      "Iteration 142, loss = 0.00452875\n",
      "Iteration 143, loss = 0.00444839\n",
      "Iteration 144, loss = 0.00437023\n",
      "Iteration 145, loss = 0.00429418\n",
      "Iteration 146, loss = 0.00422023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training finished.\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73        11\n",
      "           1       0.73      0.73      0.73        11\n",
      "\n",
      "    accuracy                           0.73        22\n",
      "   macro avg       0.73      0.73      0.73        22\n",
      "weighted avg       0.73      0.73      0.73        22\n",
      "\n",
      "Confusion Matrix:\n",
      " [[8 3]\n",
      " [3 8]]\n",
      "\n",
      "Final training loss: 0.0042\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Configure your data directory\n",
    "base_dir = 'output_lexical_metrics'\n",
    "\n",
    "# 2. Verify directory structure\n",
    "print(f\"Checking data directory: {base_dir}\")\n",
    "if not os.path.isdir(base_dir):\n",
    "    raise FileNotFoundError(f\"Data directory '{base_dir}' does not exist.\")\n",
    "for subgroup in ['cc', 'cd']:\n",
    "    subgroup_path = os.path.join(base_dir, subgroup)\n",
    "    print(f\" - Subfolder '{subgroup_path}': {'FOUND' if os.path.isdir(subgroup_path) else 'MISSING'}\")\n",
    "    if not os.path.isdir(subgroup_path):\n",
    "        raise FileNotFoundError(f\"Expected subfolder '{subgroup_path}' not found.\")\n",
    "\n",
    "# 3. Load CSV metrics and labels\n",
    "print(\"\\nLoading CSV files...\")\n",
    "data_rows, labels = [], []\n",
    "for label in ['cc', 'cd']:\n",
    "    folder = os.path.join(base_dir, label)\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith('.csv')]\n",
    "    print(f\" * {len(files)} files in '{label}'\")\n",
    "    for fname in files:\n",
    "        df = pd.read_csv(os.path.join(folder, fname))\n",
    "        data_rows.append(df.iloc[0])\n",
    "        labels.append(label)\n",
    "\n",
    "# 4. Build DataFrame\n",
    "df = pd.DataFrame(data_rows)\n",
    "df['label'] = labels\n",
    "print(f\"\\nTotal samples: {len(df)} (cc={df.label.value_counts()['cc']}, cd={df.label.value_counts()['cd']})\")\n",
    "\n",
    "# 5. Separate features/target and check for NaNs\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label'].map({'cc': 0, 'cd': 1})\n",
    "\n",
    "print(\"\\nMissing values per feature:\")\n",
    "missing = X.isnull().sum()\n",
    "print(missing[missing > 0] if missing.any() else \"No missing values!\")\n",
    "\n",
    "# 6. Impute missing values with KNNImputer\n",
    "if missing.any():\n",
    "    print(\"\\nImputing missing values using KNNImputer (k=5)...\")\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "    print(\"Imputation complete. No more missing values:\",\n",
    "          X.isnull().sum().sum() == 0)\n",
    "\n",
    "# 7. Train/test split\n",
    "print(\"\\nSplitting data (80% train, 20% test)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\" Training samples: {X_train.shape[0]}\")\n",
    "print(f\" Test samples:     {X_test.shape[0]}\")\n",
    "\n",
    "# 8. Standardize features\n",
    "print(\"\\nStandardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "print(\"Standardization complete.\")\n",
    "\n",
    "# 9. Initialize and train MLP (with verbose output)\n",
    "print(\"\\nInitializing MLPClassifier...\")\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 10. Evaluate\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\nFinal training loss: {mlp.loss_curve_[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
