{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047599f0-a379-4693-b0ff-9d0f47b4899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data directory: output_lexical_metrics\n",
      " - Subfolder 'output_lexical_metrics/cc': FOUND\n",
      " - Subfolder 'output_lexical_metrics/cd': FOUND\n",
      "\n",
      "Loading lexical metrics CSVs...\n",
      " * 54 in 'cc'\n",
      " * 54 in 'cd'\n",
      "Total lexical samples: 108\n",
      "\n",
      "Loading MMSE scores from: mmse_input.csv\n",
      "Original MMSE columns: ['filename', 'mmse_score']\n",
      "MMSE data after processing filenames:\n",
      "     filename  mmse_score\n",
      "0  audio_S001          29\n",
      "1  audio_S002          30\n",
      "2  audio_S003          29\n",
      "3  audio_S004          30\n",
      "4  audio_S005          30\n",
      "\n",
      "After merge, data shape: (108, 59)\n",
      "MMSE missing for 1 samples\n",
      "\n",
      "Imputing missing values via column means...\n",
      "Missing after imputation: 0\n",
      "\n",
      "Splitting data (80% train / 20% test)…\n",
      "Standardizing features…\n",
      "\n",
      "Initializing and training MLPClassifier…\n",
      "Iteration 1, loss = 0.69367340\n",
      "Iteration 2, loss = 0.64126412\n",
      "Iteration 3, loss = 0.59744735\n",
      "Iteration 4, loss = 0.56103335\n",
      "Iteration 5, loss = 0.53043370\n",
      "Iteration 6, loss = 0.50451644\n",
      "Iteration 7, loss = 0.48199054\n",
      "Iteration 8, loss = 0.46158795\n",
      "Iteration 9, loss = 0.44276803\n",
      "Iteration 10, loss = 0.42474352\n",
      "Iteration 11, loss = 0.40734474\n",
      "Iteration 12, loss = 0.39041116\n",
      "Iteration 13, loss = 0.37422194\n",
      "Iteration 14, loss = 0.35852167\n",
      "Iteration 15, loss = 0.34308630\n",
      "Iteration 16, loss = 0.32826424\n",
      "Iteration 17, loss = 0.31403896\n",
      "Iteration 18, loss = 0.30040244\n",
      "Iteration 19, loss = 0.28734075\n",
      "Iteration 20, loss = 0.27459811\n",
      "Iteration 21, loss = 0.26247962\n",
      "Iteration 22, loss = 0.25103616\n",
      "Iteration 23, loss = 0.24004429\n",
      "Iteration 24, loss = 0.22945176\n",
      "Iteration 25, loss = 0.21916960\n",
      "Iteration 26, loss = 0.20908800\n",
      "Iteration 27, loss = 0.19923450\n",
      "Iteration 28, loss = 0.18978318\n",
      "Iteration 29, loss = 0.18068573\n",
      "Iteration 30, loss = 0.17180222\n",
      "Iteration 31, loss = 0.16317848\n",
      "Iteration 32, loss = 0.15486506\n",
      "Iteration 33, loss = 0.14691946\n",
      "Iteration 34, loss = 0.13929809\n",
      "Iteration 35, loss = 0.13198516\n",
      "Iteration 36, loss = 0.12497933\n",
      "Iteration 37, loss = 0.11823433\n",
      "Iteration 38, loss = 0.11173646\n",
      "Iteration 39, loss = 0.10547212\n",
      "Iteration 40, loss = 0.09952226\n",
      "Iteration 41, loss = 0.09386203\n",
      "Iteration 42, loss = 0.08848733\n",
      "Iteration 43, loss = 0.08338634\n",
      "Iteration 44, loss = 0.07856295\n",
      "Iteration 45, loss = 0.07400265\n",
      "Iteration 46, loss = 0.06968605\n",
      "Iteration 47, loss = 0.06562904\n",
      "Iteration 48, loss = 0.06181086\n",
      "Iteration 49, loss = 0.05821939\n",
      "Iteration 50, loss = 0.05484786\n",
      "Iteration 51, loss = 0.05166992\n",
      "Iteration 52, loss = 0.04868895\n",
      "Iteration 53, loss = 0.04588509\n",
      "Iteration 54, loss = 0.04323413\n",
      "Iteration 55, loss = 0.04072836\n",
      "Iteration 56, loss = 0.03838551\n",
      "Iteration 57, loss = 0.03619433\n",
      "Iteration 58, loss = 0.03414739\n",
      "Iteration 59, loss = 0.03222999\n",
      "Iteration 60, loss = 0.03042806\n",
      "Iteration 61, loss = 0.02874329\n",
      "Iteration 62, loss = 0.02716416\n",
      "Iteration 63, loss = 0.02569061\n",
      "Iteration 64, loss = 0.02431496\n",
      "Iteration 65, loss = 0.02303061\n",
      "Iteration 66, loss = 0.02183063\n",
      "Iteration 67, loss = 0.02070665\n",
      "Iteration 68, loss = 0.01965327\n",
      "Iteration 69, loss = 0.01867120\n",
      "Iteration 70, loss = 0.01775274\n",
      "Iteration 71, loss = 0.01689074\n",
      "Iteration 72, loss = 0.01608361\n",
      "Iteration 73, loss = 0.01533124\n",
      "Iteration 74, loss = 0.01462836\n",
      "Iteration 75, loss = 0.01397204\n",
      "Iteration 76, loss = 0.01335704\n",
      "Iteration 77, loss = 0.01277707\n",
      "Iteration 78, loss = 0.01223139\n",
      "Iteration 79, loss = 0.01171842\n",
      "Iteration 80, loss = 0.01123500\n",
      "Iteration 81, loss = 0.01078013\n",
      "Iteration 82, loss = 0.01035064\n",
      "Iteration 83, loss = 0.00994595\n",
      "Iteration 84, loss = 0.00956378\n",
      "Iteration 85, loss = 0.00920244\n",
      "Iteration 86, loss = 0.00886275\n",
      "Iteration 87, loss = 0.00854103\n",
      "Iteration 88, loss = 0.00823558\n",
      "Iteration 89, loss = 0.00794697\n",
      "Iteration 90, loss = 0.00767362\n",
      "Iteration 91, loss = 0.00741336\n",
      "Iteration 92, loss = 0.00716634\n",
      "Iteration 93, loss = 0.00693181\n",
      "Iteration 94, loss = 0.00670952\n",
      "Iteration 95, loss = 0.00649929\n",
      "Iteration 96, loss = 0.00629907\n",
      "Iteration 97, loss = 0.00610803\n",
      "Iteration 98, loss = 0.00592585\n",
      "Iteration 99, loss = 0.00575232\n",
      "Iteration 100, loss = 0.00558643\n",
      "Iteration 101, loss = 0.00542836\n",
      "Iteration 102, loss = 0.00527777\n",
      "Iteration 103, loss = 0.00513354\n",
      "Iteration 104, loss = 0.00499541\n",
      "Iteration 105, loss = 0.00486240\n",
      "Iteration 106, loss = 0.00473520\n",
      "Iteration 107, loss = 0.00461343\n",
      "Iteration 108, loss = 0.00449709\n",
      "Iteration 109, loss = 0.00438533\n",
      "Iteration 110, loss = 0.00427748\n",
      "Iteration 111, loss = 0.00417403\n",
      "Iteration 112, loss = 0.00407477\n",
      "Iteration 113, loss = 0.00397918\n",
      "Iteration 114, loss = 0.00388707\n",
      "Iteration 115, loss = 0.00379832\n",
      "Iteration 116, loss = 0.00371274\n",
      "Iteration 117, loss = 0.00363033\n",
      "Iteration 118, loss = 0.00355093\n",
      "Iteration 119, loss = 0.00347435\n",
      "Iteration 120, loss = 0.00340047\n",
      "Iteration 121, loss = 0.00332917\n",
      "Iteration 122, loss = 0.00326028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Evaluating on test set…\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       0.77      0.91      0.83        11\n",
      "\n",
      "    accuracy                           0.82        22\n",
      "   macro avg       0.83      0.82      0.82        22\n",
      "weighted avg       0.83      0.82      0.82        22\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 8  3]\n",
      " [ 1 10]]\n",
      "\n",
      "Final training loss: 0.0033\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "base_dir = 'output_lexical_metrics'            \n",
    "mmse_path = 'mmse_input.csv'          \n",
    "\n",
    "print(f\"Checking data directory: {base_dir}\")\n",
    "if not os.path.isdir(base_dir):\n",
    "    raise FileNotFoundError(f\"Data directory '{base_dir}' not found.\")\n",
    "for subgroup in ['cc', 'cd']:\n",
    "    path = os.path.join(base_dir, subgroup)\n",
    "    print(f\" - Subfolder '{path}': {'FOUND' if os.path.isdir(path) else 'MISSING'}\")\n",
    "    if not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"Expected '{path}' not found.\")\n",
    "\n",
    "print(\"\\nLoading lexical metrics CSVs...\")\n",
    "filenames, data_rows, labels = [], [], []\n",
    "for label in ['cc', 'cd']:\n",
    "    folder = os.path.join(base_dir, label)\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith('.csv')]\n",
    "    print(f\" * {len(files)} in '{label}'\")\n",
    "    for fname in files:\n",
    "        df = pd.read_csv(os.path.join(folder, fname))\n",
    "        data_rows.append(df.iloc[0])\n",
    "        labels.append(label)\n",
    "        # strip extension to match mmse filenames\n",
    "        base_name = os.path.splitext(fname)[0]\n",
    "        filenames.append(base_name)\n",
    "\n",
    "lex_df = pd.DataFrame(data_rows)\n",
    "lex_df['label'] = labels\n",
    "lex_df['filename'] = filenames\n",
    "print(f\"Total lexical samples: {len(lex_df)}\")\n",
    "\n",
    "#  Load and preprocess MMSE scores\n",
    "print(f\"\\nLoading MMSE scores from: {mmse_path}\")\n",
    "mmse_df = pd.read_csv(mmse_path)\n",
    "print(\"Original MMSE columns:\", mmse_df.columns.tolist())\n",
    "\n",
    "# Ensure 'filename' and 'mmse_score' columns exist\n",
    "# and strip .csv if included\n",
    "if 'filename' not in mmse_df.columns:\n",
    "    raise ValueError(\"MMSE file must have a 'filename' column.\")\n",
    "mmse_df['filename'] = mmse_df['filename'].apply(lambda x: os.path.splitext(x)[0])\n",
    "if 'mmse_score' not in mmse_df.columns:\n",
    "    raise ValueError(\"MMSE file must have a 'mmse_score' column.\")\n",
    "\n",
    "print(\"MMSE data after processing filenames:\")\n",
    "print(mmse_df.head())\n",
    "\n",
    "# 5. Merge lexical metrics with MMSE\n",
    "merged = pd.merge(lex_df, mmse_df, on='filename', how='left')\n",
    "print(f\"\\nAfter merge, data shape: {merged.shape}\")\n",
    "missing_mmse = merged['mmse_score'].isna().sum()\n",
    "print(f\"MMSE missing for {missing_mmse} samples\")\n",
    "\n",
    "# 6. Prepare feature matrix and target\n",
    "X = merged.drop(columns=['label', 'filename'])\n",
    "y = merged['label'].map({'cc': 0, 'cd': 1})\n",
    "\n",
    "# 7. Impute missing values with mean\n",
    "print(\"\\nImputing missing values via column means...\")\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "print(\"Missing after imputation:\", X.isna().sum().sum())\n",
    "\n",
    "# 8. Split, standardize, and train MLP\n",
    "print(\"\\nSplitting data (80% train / 20% test)…\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Standardizing features…\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nInitializing and training MLPClassifier…\")\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 9. Evaluate\n",
    "print(\"\\nEvaluating on test set…\")\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\nFinal training loss: {mlp.loss_curve_[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
